# -*- coding: utf-8 -*-
"""multi_class_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JeBmHeoysnbM8wCIqdzY9FJNsiK1kyN6
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split as split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import itertools
from tensorflow.keras.datasets import fashion_mnist

(train_data,train_labels),(test_data,test_labels)=fashion_mnist.load_data()

len(test_data)

train_data[0],train_labels[0]

train_data[0].shape,train_labels[0].shape

plt.imshow(train_data[0], cmap=plt.cm.binary)

class_names=['T-shirt/top','Trouser',	'Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']

class_names[train_labels[0]]

plt.figure(figsize=(7,7))
for i in range(10):
  ax=plt.subplot(2,5,i+1)
  index=random.choice(range(len(train_data)))
  plt.imshow(train_data[index], cmap=plt.cm.binary)
  plt.title(class_names[train_labels[index]])

tf.random.set_seed(42)

model=tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28,28)),
    #tf.keras.layers.InputLayer(shape=(28,28)),
    tf.keras.layers.Dense(4,activation="relu"),
    tf.keras.layers.Dense(4,activation="relu"),
   #tf.keras.layers.Dense(28,activation="relu"),
    tf.keras.layers.Dense(10,activation="softmax")
])

model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),
              optimizer=tf.keras.optimizers.Adam(),
              metrics=["accuracy"])


# can use categoricalcrossentropy if labels are one hot encoded
# tf.one_hot(train_labels,depth=10) also do test_labels

# early_stopping = tf.keras.callbacks.EarlyStopping(
#     monitor='loss',
#     patience=10,
#     restore_best_weights=True,
#     verbose=1
# )

# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
#     monitor='loss',
#     factor=0.5,
#     patience=3,
#     verbose=1
# )


# lr_schedule=tf.keras.callbacks.LearningRateScheduler(
#     lambda epoch :( 1e-4 * 10**(epoch/20))
# )



non_norm_hist=model.fit(train_data,
                        tf.one_hot(train_labels,depth=10),
                        epochs=10,
                        validation_data=(test_data,tf.one_hot(test_labels,depth=10))
                        )

model.summary()

train_data_norm=train_data/255.0
test_data_norm=test_data/255.0

train_data_norm.min(),train_data_norm.max()

tf.random.set_seed(42)

model_2=tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28,28)),
    #tf.keras.layers.InputLayer(shape=(28,28)),
    tf.keras.layers.Dense(32,activation="relu"),
    tf.keras.layers.Dense(32,activation="relu"),
    tf.keras.layers.Dense(16,activation="relu"),
    tf.keras.layers.Dense(10,activation="softmax")
])

model_2.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              metrics=["accuracy"])


# can use categoricalcrossentropy if labels are one hot encoded
# tf.one_hot(train_labels,depth=10) also do test_labels

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='accuracy',
    patience=2,
    restore_best_weights=True,
    verbose=1
)

# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
#     monitor='loss',
#     factor=0.5,
#     patience=3,
#     verbose=1
# )


# lr_schedule=tf.keras.callbacks.LearningRateScheduler(
#     lambda epoch :( 1e-3 * 10**(epoch/20))
# )



norm_hist=model_2.fit(train_data_norm,
                       train_labels,
                      callbacks=[early_stopping],
                        epochs=30,
                        validation_data=(test_data_norm,test_labels)
                        )

# learning rate decay curve
lrs= 1e-3 * (10**(tf.range(10)/20))
plt.figure(figsize=(10,7))
plt.semilogx(lrs,norm_hist.history["loss"])
plt.xlabel("lr")
plt.ylabel("loss")

pd.DataFrame(non_norm_hist.history).plot()
pd.DataFrame(norm_hist.history).plot()

def make_confusion_matrix(y_true,y_pred,classes=None,figsize=(10,10),text_size=15):

  cm=confusion_matrix(y_true,y_pred)
  cm_norm=cm.astype("float")/cm.sum(axis=1)[:,np.newaxis]
  n_classes=cm.shape[0]


  fig,ax=plt.subplots(figsize=figsize)

  cax=ax.matshow(cm,cmap=plt.cm.Blues)
  fig.colorbar(cax)


  if classes:
    labels=classes
  else:
    labels=np.arange(cm.shape[0])

  ax.set(title="confusion matrix",
        xlabel="predicted",
        ylabel="true",
        xticks=np.arange(n_classes),
        yticks=np.arange(n_classes),
        xticklabels=labels,
        yticklabels=labels)

  ax.xaxis.set_label_position('bottom')
  ax.xaxis.tick_bottom()

  ax.yaxis.label.set_size(text_size)
  ax.xaxis.label.set_size(text_size)
  ax.title.set_size(text_size)

  threshold = (cm.max()+cm.min())/2

  for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):
    plt.text(j,i,f"{cm[i,j]}  ({cm_norm[i,j]*100:.1f}%)",
            horizontalalignment='center',
            color='white' if cm[i,j]>threshold else 'black',
            size=text_size
            )

pred=model_2.predict(test_data_norm)
pred[0]

print(pred[0].max())
class_names[tf.argmax(pred[0])]==class_names[test_labels[0]]
#tf.argmax returns index of highest element

prediction=pred.argmax(axis=1)
prediction

test_labels

print(classification_report(test_labels, prediction))

make_confusion_matrix(test_labels,prediction,class_names,(28,28))

def plot_random_image(model,images,true_labels,classes):

  i=random.randint(0,len(images))
  target_image=images[i]
  pred_probs=model.predict(target_image.reshape(1,28,28))
  pred_label=classes[pred_probs.argmax()]
  true_label=classes[true_labels[i]]

  plt.imshow(target_image,cmap=plt.cm.binary)

  if pred_label==true_label:
    color='green'
  else:
    color='red'

  plt.xlabel("pred: {} {:2.0f}%  (true: {} )".format(pred_label,
                                               100*tf.reduce_max(pred_probs),
                                               true_label),
                                               color=color)
  print("image number ",i)

plot_random_image(model_2,test_data_norm,test_labels,class_names)

model_2.summary()

model_2.layers[1]

weights,biases=model_2.layers[1].get_weights()

weights,weights.shape

biases,biases.shape

